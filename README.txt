ECE 8813/ CS 8803 Advance Computer Security for student to learn to improve skills

Project Title: Anomaly Detection by passive DNS analysis of Alexa Domains
Team Members: Ayush Sharma, Harshitha Ramamurthy, Hirva Shah, Pragna Hasthanthar Anand

This system has been designed to raise a red flag in case of DNS Poisoning and malicious DNS authority by passive DNS analysis of domain names and then comparing them with the zeroth day cluster of the local database that has been created to store DNS information.

How to run the files:

1. Generation Of Features Using a DNS Crawler

$python dns_crawler_feature_generation.py <input domains_without http> [<output_file_name.csv>]

Run the python dns_crawler_feature_generation.py as shown above by giving an input file with a list of domain names (ex: google.com; we have used Alexa's list for this), specifying an output file name for the csv generated (default name is weka_features_final.csv). This output file generated by the dns resolver should be given as an input to the weka classifier for clustering. The dns resolver generates intermediate output csv files which contain the raw DNS data extracted. These files can be located with file names- features_dns_db_multi<process_id>.csv 

2. Cluster Comparison

$python compare_clusters.py <output file name> <input file1> [<input file2>...]

The output of the weka classifier is given as an input to compare_clusters.py which generates an output file showing changes in clusters for each domain. The script can also take multiple input files to the script to compare the clusters instead of a single combined csv file.


3.Alexa Categories Crawler

The system must have scrapy installed and the existing items.py and pipelines.py replaced with those which are provided before running the below command.

$scrapy crawl spider_alexa

spider_alexa is the name of the spider. The items.py is to define the alexa item for each domain and pipelines.py produces a list of the required domains in required format as a csv file.
The spider crawls the alexa website and generates the list of 500 domains of alexa categories for each category. 

4. Retrieving Websites Using Pycurl (for generating DNS traffic)

$python url_retriever_and_pcap_generator.py <input file>

Run the above command by giving a list of URLS (required format: http://google.com/) in a input file to retrieve the webpages for the givens URLs.
A packet capture file containing DNS traffic is generated with the filename pcap_dns_data.pcap.
The webpages are stored as doc_<index number>.dat files.


5. Extracting Features From Pcap Files

$python extracting_features_from_pcap.py <input domains> 

Run the above command to extract data from the pcap files and store it in a csv format.
The input pcap file must be specified as a command line argument. The output file name is features_pcap.csv
